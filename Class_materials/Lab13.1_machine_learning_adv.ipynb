{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hairy-certificate",
   "metadata": {},
   "source": [
    "Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-democrat",
   "metadata": {},
   "source": [
    "Labpartner(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-camcorder",
   "metadata": {},
   "source": [
    "# Class 13.1 (And 13.2? Let's see how long this takes...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-telescope",
   "metadata": {},
   "source": [
    "#### Announcements\n",
    "\n",
    "- Reminder **no labs will be accepted after 4/18.** After this you all will be working on your final projects. Please review the syllabus on BB for grading policies. \n",
    "- For the next two weeks we will have virtual workdays from 2-4PM, and you will submit your programming and a weekly project planning and assessment form.\n",
    "- If there is a topic you all are excited about for Wed, let me know\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-institute",
   "metadata": {},
   "source": [
    "# Warmups 13.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-messaging",
   "metadata": {},
   "source": [
    "Standard error (SE), aslo called root mean squre error (RMSE), is a measure of how \"off\" the model is from the data, and is calculated as the squared sum of the difference between the model and the data, divided by the number of data points, then the squre root is taken:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-deficit",
   "metadata": {},
   "source": [
    "###   SE = $ \\sqrt{\\frac {\\Sigma ( y_{data}-y_{model} )^2}{N}} $\n",
    "\n",
    "where $y_{data}$ is the data to be fit by a model, $y_{model}$ is the model prediction, N is the number of data points. $\\Sigma$ denotes a sum over all the data points. See https://onlinestatbook.com/2/regression/accuracy.html for more info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-fountain",
   "metadata": {},
   "source": [
    "**W.1** Revisit the linear fit for the Boston housing data set (Lab 10.2), where you did the linear model of housing price vs. number of rooms and also lower status of the population. Compare and contrast the correlation, p values, and standard error for these two linear models. How does the standard error inform your decition as to which variable is a better predictor of housing value? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-philip",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "macro-charleston",
   "metadata": {},
   "source": [
    "**W.2** Using the statsmodels package, add a second variable to your model, so that you are fitting the equation:\n",
    "\n",
    "$ MEDV = a*RM + b*LSTAT +c$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-personal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "convinced-chambers",
   "metadata": {},
   "source": [
    "**W.3** Is this a more predictive model of housing value? Explain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-envelope",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "blond-business",
   "metadata": {},
   "source": [
    "**W.4** What are the values of a and b? What do they mean in this context? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-forge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-brother",
   "metadata": {},
   "source": [
    "# Lecture 13.1\n",
    "\n",
    "## Agenda:\n",
    "- Machine Learning in Scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-grave",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Machine Learning has emerged as an important tool accross the scientific diciplines and in many business applications. The most commonly used package for machine learning is scikit-learn, which we will play around with this week.\n",
    "\n",
    "There are a number of types of machine learning algorithms, aka \"estimators\". A useful flowchart:\n",
    "https://scikit-learn.org/stable/tutorial/machine_learning_map/\n",
    "\n",
    "These can be divided up into:\n",
    "\n",
    "1) Regression. This involves finding the best fit parameters for some model, like with did with linear models.\n",
    "\n",
    "2) Clustering, which groups data by some similarity criterion (e.g., these data points are grass, these are trees, and these are water because of their spectral data differences). See https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py\n",
    "\n",
    "3) Dimensionality reduction, which breaks data, often timesries data, into principle components. For example if we were looking at a temperature timeseries, we might get as components an annual cycle, a daily cycle, and a multi-year cycle corresponding to ENSO (El Nino - Southern Oscillation), etc.\n",
    "\n",
    "4) Classification. This uses labeled data to make preditions about what will happen in places you don't have data, for example in this area it is expected to see large green birds and not small red birds, and in another area it is expected to see both types. We will focus on this today.\n",
    "\n",
    "See also: https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-adaptation",
   "metadata": {},
   "source": [
    "## Classification example from scikit-learn\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-blues",
   "metadata": {},
   "source": [
    "Here is the whole code below. We are going to break this example down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9), facecolor = 'w')\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   edgecolors='green', alpha=1, zorder = 10)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name, fontsize = 14)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-advocacy",
   "metadata": {},
   "source": [
    "# Step by step\n",
    "\n",
    "### Load all the packages in\n",
    "\n",
    "Note they do this for each package separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# data and pre-proccessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "# all the classifiers we will use:\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "# create a string list of all the cassifier names\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function list of all the classifiers\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-disco",
   "metadata": {},
   "source": [
    "## Part 1: Make some datasets to classify\n",
    "\n",
    "The example uses three types of data, linearly separable, moons and circles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-suffering",
   "metadata": {},
   "source": [
    "### Data 1. Linearly separable (side by side data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some data to play with (this is from sklearn.datasets, loaded above)\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1], c= y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add som noise to these so they smoosh together\n",
    "X_old = np.copy(X)\n",
    "# make a random number object\n",
    "rng = np.random.RandomState(2)\n",
    "# add some random numbers to the data\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_old[:,0],X_old[:,1], c = y)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1], c = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-ordering",
   "metadata": {},
   "source": [
    "### E1. What happens if you use X_old = X instead of X_old = np.copy(X)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-lingerie",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "reported-affect",
   "metadata": {},
   "source": [
    "### Data 2. Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_moons(noise=0.3, random_state=10)\n",
    "plt.scatter(X[:,0],X[:,1], c = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-northern",
   "metadata": {},
   "source": [
    "### E2. What happens to the make_moons data if you change the noise and random state? Why do you think they picked noise = 0.3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-ecology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "numeric-jerusalem",
   "metadata": {},
   "source": [
    "### Data 3. Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some data to play with (this is from sklearn.datasets, loaded above)\n",
    "X, y = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1], c= y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-default",
   "metadata": {},
   "source": [
    "### E3. Play around with the options for make_circles. What does factor do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-effects",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the three types of datasets together in one array\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-secret",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adverse-minute",
   "metadata": {},
   "source": [
    "## Part 2: Big double for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-ranking",
   "metadata": {},
   "source": [
    "The next part of the code has a double for loop that:\n",
    "\n",
    "    Loops over the datasets\n",
    "    # iterate over datasets\n",
    "    for ds_cnt, ds in enumerate(datasets):\n",
    "    \n",
    "        Loops over the classifiers\n",
    "        # iterate over classifiers\n",
    "        for name, clf in zip(names, classifiers):\n",
    "        \n",
    "        \n",
    "Let's break this down, and start with one dataset and apply a number of classifiers to it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-lodge",
   "metadata": {},
   "source": [
    "### Using first dataset (moons) and first estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-national",
   "metadata": {},
   "source": [
    "#### FIrst we preproccess\n",
    "preprocess dataset, split into training and test part\n",
    "\n",
    "scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first dataset, which is moons\n",
    "X, y = datasets[0]\n",
    "\n",
    "# scale the data by the mean and standard deviation, i.e. z = (x - u) / s, \n",
    "# where x is the data, u is the mean, and s is the standard deviation\n",
    "# see: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "X = StandardScaler().fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-brazil",
   "metadata": {},
   "source": [
    "### E4 make a plot with two subplots, showing the scaled and unscaled data next to each other. How are they different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-basketball",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=.4, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train # color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train #position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-webcam",
   "metadata": {},
   "source": [
    "### E5 What percentage of the data is used for training here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-bunch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "certified-collins",
   "metadata": {},
   "source": [
    "### plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# set up a meshgrid for plotting the classification result based on the size of the dataset\n",
    "# note this will be used later\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "#######\n",
    "\n",
    "# counter for which subplot we are in\n",
    "i = 1\n",
    "\n",
    "# this counter is used to know when to plot the title\n",
    "ds_cnt = 0\n",
    "\n",
    "# just plot the dataset first\n",
    "\n",
    "# note this is not used yet\n",
    "cm = plt.cm.RdBu\n",
    "\n",
    "# set the colormap used for the data \n",
    "# see https://matplotlib.org/tutorials/colors/colorbar_only.html#sphx-glr-tutorials-colors-colorbar-only-py\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "ax = plt.subplot(1,1, i)\n",
    "#ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "\n",
    "# if the first row, plot the title\n",
    "if ds_cnt == 0:\n",
    "    ax.set_title(\"Input data\")\n",
    "    \n",
    "# Plot the training points using the bright colormap\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k')\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.3,\n",
    "           edgecolors='k')\n",
    "\n",
    "# set the limits to the colormap min max we will use later\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "\n",
    "# get rid of ticks\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "\n",
    "# increment to go to the next subplot\n",
    "i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-invasion",
   "metadata": {},
   "source": [
    "Note the testing data is the more transparent set, and the training data is darker\n",
    "\n",
    "Now we are ready to apply classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-wisconsin",
   "metadata": {},
   "source": [
    "Remember we are doing the following:\n",
    "\n",
    "classifiers = [\n",
    "\n",
    "    KNeighborsClassifier(3),\n",
    "    \n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    \n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    \n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    \n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    \n",
    "    MLPClassifier(alpha=1),\n",
    "    \n",
    "    AdaBoostClassifier(),\n",
    "    \n",
    "    GaussianNB(),\n",
    "    \n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-rouge",
   "metadata": {},
   "source": [
    "## Nearest neighbors\n",
    "Basically this method looks at k number of nearest neighbors, using some distance metric, and predicts which class a point is in based on the type of thing those neighbors are.\n",
    "\n",
    "intro https://www.python-course.eu/k_nearest_neighbor_classifier.php\n",
    "\n",
    "see https://scikit-learn.org/stable/modules/neighbors.html\n",
    "\n",
    "and https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "\n",
    "actual help files:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first classifier, KNeighborsClassifier(3)\n",
    "# here we are using 3 nearest neighbors\n",
    "\n",
    "# index for classifier\n",
    "c = 0\n",
    "\n",
    "name = names[0]\n",
    "clf = classifiers[0]\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf\n",
    "\n",
    "# weights = 'uniform' here means we are not weighting by distance\n",
    "# p=2 is using Euclidean (standard) distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-attendance",
   "metadata": {},
   "source": [
    "Next we will plot the decision boundary using the mesh we generated. Basically this is asking, for each point on the plane, which group would I be in, red or blue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "if hasattr(clf, \"decision_function\"):\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "else:\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-fence",
   "metadata": {},
   "source": [
    "### E6 what does the xx.ravel() bit of code do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-avenue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-composition",
   "metadata": {},
   "source": [
    "Note that kneighbors does not have a decision_function, so we are using predict_proba. This is the probability that each point in the meshgrid is in either the red class (zeros) or the blue class (ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "# note we are using the meshgrid we created before to plot this as a filled contour plot\n",
    "plt.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-white",
   "metadata": {},
   "source": [
    "### E7 what do the light blue and peach colors represent?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-steel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "operating-median",
   "metadata": {},
   "source": [
    "Now we will add in the training and testing points, and see how well the method did to predict the latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "# note we are using the meshgrid we created before to plot this as a filled contour plot\n",
    "plt.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k')\n",
    "# Plot the testing points\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "           edgecolors='orange', alpha=1, zorder = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-roommate",
   "metadata": {},
   "source": [
    "### E8 How well did the method do? Are there outliers in either class? Is it reasonable that this method did not predict those, given the training dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-accident",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cross-stage",
   "metadata": {},
   "source": [
    "### E9 Using one of the other data sets and methods, go through the same exercise as above. Explain the basics of what your method does, and how well it works for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-british",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "occupational-establishment",
   "metadata": {},
   "source": [
    "### E10 Again, using one of the other data sets and methods, go through the same exercise as above. Explain the basics of what your method does, and how well it works for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-research",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-humidity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "simplified-cabinet",
   "metadata": {},
   "source": [
    "## Notes on clustering vs. classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-intervention",
   "metadata": {},
   "source": [
    "Note that clustering is similar, but we don't have the y data, i.e. red/blue or any type of class assignemnt in the data itself, just locations in some variable space (could be lat/lon, or could be something way more abstract, like frequency vs. intensity space for sound data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-salad",
   "metadata": {},
   "source": [
    "Here is a clustering example, similar to what we did above:\n",
    "https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-broadcast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-baltimore",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
